#
# This file contains configuration flags to customize your site
#

# Name of your site (displayed in the header)
name: Algorithms for Convex Optimization

# Short bio or description (displayed in the header)
description: Convex optimization studies the problem of minimizing convex functions over convex sets. Convexity, and its numerous implications, has been used to come up with efficient algorithms for many classes of convex programming problems. Consequently, this area has broadly impacted science and engineering.<br> <br> In the last few years, convex optimization methods have revolutionized algorithm design, both for discrete and continuous optimization problems. The fastest known algorithms for problems such as maximum flow in graphs, maximum matching in bipartite graphs, and submodular function minimization involve an essential and nontrivial use of convex optimization techniques such as gradient descent, mirror descent, accelerated gradient descent, interior point methods, and cutting plane methods. Convex optimization methods have also been used to design counting problems over discrete objects such as spanning trees. On the other hand, convex optimization methods are at the core of many modern machine learning algorithms. The demand for convex optimization methods, driven by larger and increasingly complex input instances, has also significantly pushed the state of the art of convex optimization itself.  <br> <br> The goal of this book is to enable a reader to gain an in depth understanding of convex optimization methods. The emphasis is on deriving various convex optimization methods in from “first principles” and establishing precise running time bounds in terms of the input length. This book shows applications to fast algorithms for various discrete optimization and counting problems. These applications also serve the purpose of illustrating a rather surprising and, in progress, “bridge” between continuous and discrete optimization.


# URL of your avatar or profile pic (you could use your GitHub profile pic)
avatar: images/blank.png

pdf: ACO.pdf


author: Nisheeth K. Vishnoi
authorWebsite: https://www.cs.yale.edu/homes/vishnoi/Home.html

chapters:
    - number: 1
      title: Bridging continuous and discrete optimization
      description: This chapter presents the interplay between continuous and discrete optimization. The motivating example is that of the maximum flow problem. It also traces the history of linear programming -- from the ellipsoid method to modern interior point methods. It ends with some of the recent successes of the ellipsoid method for general convex programming problems such as the maximum entropy problem. 
      
    - number: 2
      title: Preliminaries
      description: In this chapter we review the basic mathematical preliminaries and tools required for this book. These include some standard notion and facts from multivariate calculus, linear algebra, geometry, topology, dynamical systems, and graph theory.


    - number: 3
      title: Convexity 
      description: In this chapter we introduce convex sets, various notions of convexity, and show the power that comes along with convexity: convex sets have separating hyperplanes, sub- gradients exist, and locally optimal solutions of convex functions are globally optimal.
      
    - number: 4
      title: Convex Programming and Efficiency 
      description: In this chapter we formalize the convex programming problem and discuss what it means to solve it efficiently as a function of the representation length of the input and the accuracy.
      
    - number: 5
      title: Duality and Optimality
      description: In this chapter we introduce the powerful notion of Lagrangian duality. We show that under a mild condition, called Slater’s condition, strong Lagrangian duality holds. We introduce the Legendre-Fenchel dual that often arises in Lagrangian duality and opti- mization methods. Finally, we mention the KKT optimality conditions that arise from duality.

   
    - number: 6
      title: Gradient Descent
      description: In this chapter we present gradient descent and show how it can be derived from an optimization viewpoint. Under convexity of the objective, we prove a convergence time bound when the gradient of the function is Lipschitz continuous. Subsequently, we show how to use this continuous optimization method to come up with a fast algorithm for a discrete optimization problem: computing maximum flows in a graph.
   
    - number: 7
      title: Mirror Descent and  Multiplicative Weights Update 
      description: In this chapter we derive a new optimization algorithm – called mirror descent – via a different local optimization principle. First, the mirror descent algorithm is developed for optimizing convex functions over the probability simplex. Subsequently, we show how to generalize it and, importantly, derive the multiplicative weights update (MWU) method from it. This latter algorithm is then used to develop a fast approximate algo- rithm to solve the bipartite matching problem on graphs.
   
    - number: 8
      title: Accelerated Gradient Descent
      description: In this chapter we present Nesterov’s accelerated gradient descent algorithm. This algorithm can be viewed as a hybrid of the previously introduced gradient descent and mirror descent methods. We also present an application of it to solving a linear system of equations.

   
    - number: 9
      title: Newton’s Method
      description: In this chapter we begin our journey towards designing algorithms for convex optimization whose number of iterations scale polylogarithmically with the error. As a first step, we de- rive and analyze the classic Newton’s method, which is an example of a second-order optimization method. We argue that Newton’s method can be seen as gradient descent on a Riemannian manifold, which then motivates an affinely invariant analysis of its convergence.
   
    - number: 10
      title: An Interior Point Method for Linear Programming
      description: In this chapter we build upon Newton’s method and its convergence from the previ- ous chapter to derive a polynomial time algorithm for linear programming. Key to the reduction from constrained to unconstrained optimization is the notion of barrier func- tions and the corresponding central path.
   
    - number: 11
      title: Variants of the Interior Point Method and Self-Concordance
      description: In this chapter we present various generalizations and extensions of the path following IPM for the case of linear programming. As an application, we derive a fast algorithm for the s-t-minimum cost flow problem. Sub- sequently, we introduce the notion of self-concordance and give an overview of barrier functions for polytopes and more general convex sets.
   
    - number: 12
      title: Ellipsoid Method for Linear Programming
      description: In this chapter we introduce a class of cutting plane methods for convex optimization and present an analysis of a special case, namely, the ellipsoid method. We then show how to use this ellipsoid method to solve linear programs with exponentially many constraints.

   
    - number: 13
      title: Convex Programming using the Ellipsoid Method
      description: In this chapter we show how to adapt the ellipsoid method to solve general convex programs. As applications we present a polynomial time algorithm for submodular function minimization and a polynomial time algorithm to compute maximum entropy distributions over combinatorial polytopes.

   
      

copyright: This material will be published by Cambridge University Press as Algorithms for Convex Optimization by Nisheeth K. Vishnoi. This pre-publication version is free to view and download for personal use only. Not for re-distribution, re-sale or use in derivative works. &#169; Nisheeth K. Vishnoi 2020.

errata: Feedback, corrections, and comments are welcome and should be emailed to the author. 

#
# Flags below are optional
#

# Includes an icon in the footer for each username you enter
footer-links:
  dribbble:
  email:
  facebook:
  flickr:
  github:
  instagram:
  linkedin: 
  pinterest:
  rss:
  twitter:
  stackoverflow: 
  youtube: # channel/<your_long_string> or user/<user-name>
  googleplus: # anything in your profile username that comes after plus.google.com/
  playconsole:


# Enter your Disqus shortname (not your username) to enable commenting on posts
# You can find your shortname on the Settings page of your Disqus account
disqus: 

# Enter your Google Analytics web tracking code (e.g. UA-2110908-2) to activate tracking
# google_analytics: UA-43339302-11

# Your website URL (e.g. http://amitmerchant1990.github.io or http://www.amitmerchant.com)
# Used for Sitemap.xml and your RSS feed
#url: http://www.amitmerchant.com/reverie
#enforce_ssl: https://www.amitmerchant.com/reverie

# If you're hosting your site at a Project repository on GitHub pages
# (http://yourusername.github.io/repository-name)
# and NOT your User repository (http://yourusername.github.io)
# then add in the baseurl here, like this: "/repository-name"
#baseurl: ""

#
# !! You don't need to change any of the configuration flags below !!
#

permalink: /:title/

# The release of Jekyll Now that you're using
version: v1.2.0

# Jekyll 3 now only supports Kramdown for Markdown
kramdown:
  # Use GitHub flavored markdown, including triple backtick fenced code blocks
  input: GFM
  # Jekyll 3 and GitHub Pages now only support rouge for syntax highlighting
  syntax_highlighter: rouge
  syntax_highlighter_opts:
    # Use existing pygments syntax highlighting css
    css_class: 'highlight'

# Set the Sass partials directory, as we're using @imports
sass:
  style: :expanded # You might prefer to minify using :compressed

# Use the following plug-ins
plugins:
  - jekyll-sitemap # Create a sitemap using the official Jekyll sitemap gem
  - jekyll-feed # Create an Atom feed using the official Jekyll feed gem
  - jekyll-seo-tag
  - jekyll-paginate

include: ['_pages']

paginate: 6
paginate_path: /page:num/

# Exclude these files from your production _site
exclude:
  - Gemfile
  - Gemfile.lock
  - LICENSE
  - README.md
  - CNAME

